import json
import os
import sqlite3
import tldextract  # You may need to install this package using pip
from multiprocessing import Pool
import logging

logging.basicConfig(filename='analyzer_logger.log', level=logging.INFO)


def create_database_table():
    conn = sqlite3.connect('000malware.db')
    cursor = conn.cursor()

    # Create tables
    cursor.execute('''CREATE TABLE IF NOT EXISTS services (
                      service TEXT PRIMARY KEY,
                      count INTEGER,
                      countries TEXT,
                      filenames TEXT)''')

    cursor.execute('''CREATE TABLE IF NOT EXISTS domains (
                      domain TEXT PRIMARY KEY,
                      count INTEGER,
                      countries TEXT,
                      filenames TEXT)''')

    cursor.execute('''CREATE TABLE IF NOT EXISTS summary (
                      country TEXT,
                      unique_usernames INTEGER,
                      unique_passwords INTEGER,
                      unique_emails INTEGER,
                      unique_domains INTEGER,
                      unique_services INTEGER
                      )''')

    cursor.execute('''CREATE TABLE IF NOT EXISTS counts (
                      username_count INTEGER,
                      password_count INTEGER,
                      email_count INTEGER,
                      domains_count INTEGER,
                      services_count INTEGER,
                      filename TEXT)''')

    conn.commit()
    return conn


def optimize_database_indexes(conn):
    cursor = conn.cursor()

    # Index for the 'services' table
    cursor.execute(
        'CREATE INDEX IF NOT EXISTS idx_services_service ON services (service)')
    cursor.execute(
        'CREATE INDEX IF NOT EXISTS idx_services_countries ON services (countries)')
    cursor.execute(
        'CREATE INDEX IF NOT EXISTS idx_services_filenames ON services (filenames)')

    # Index for the 'domains' table
    cursor.execute(
        'CREATE INDEX IF NOT EXISTS idx_domains_domain ON domains (domain)')
    cursor.execute(
        'CREATE INDEX IF NOT EXISTS idx_domains_countries ON domains (countries)')
    cursor.execute(
        'CREATE INDEX IF NOT EXISTS idx_domains_filenames ON domains (filenames)')

    # Index for the 'summary' table
    cursor.execute(
        'CREATE INDEX IF NOT EXISTS idx_summary_country ON summary (country)')

    # Index for the 'counts' table
    cursor.execute(
        'CREATE INDEX IF NOT EXISTS idx_counts_filename ON counts (filename)')

    conn.commit()


def parse_file(filename):
    with open(filename, 'r') as file:
        data = json.load(file)

    # Initialize containers for the data to be extracted
    service_counter = {}
    domain_counter = {}
    service_country_data = []
    unique_usernames = set()
    unique_passwords = set()
    unique_emails = set()
    unique_domains = set()

    # Parse the data
    country = data.get('country', 'Unknown')

    # Count services and collect country data
    for service in data.get('services', []):
        service_counter[service] = service_counter.get(service, 0) + 1
        service_country_data.append((service, country))

    # Count domains and collect country data
    for domain in data.get('domain_names', []):
        # Extract the parent domain using tldextract
        domain_parts = tldextract.extract(domain)
        parent_domain = f"{domain_parts.domain}.{domain_parts.suffix}"

        # Update the domain counter
        if parent_domain in domain_counter:
            domain_counter[parent_domain][0] += 1
            if country not in domain_counter[parent_domain][1]:
                domain_counter[parent_domain][1].append(country)
        else:
            domain_counter[parent_domain] = [1, [country]]

    # Collect unique usernames, passwords, emails, and domains
    unique_usernames.update(data.get('usernames', []))
    unique_passwords.update(data.get('passwords', []))
    unique_emails.update(data.get('email_addresses', []))
    unique_domains.update(domain_counter.keys())

    # Prepare the parsed data for return
    parsed_data = {
        'service_counter': service_counter,
        'domain_counter': domain_counter,
        'service_country_data': service_country_data,
        'summary': {
            'country': country,
            'unique_usernames': len(unique_usernames),
            'unique_passwords': len(unique_passwords),
            'unique_emails': len(unique_emails),
            'unique_domains': len(unique_domains),
            'unique_services': len(service_counter)
        },
        'filename': os.path.basename(filename)
    }

    return parsed_data


def insert_services_data(conn, parsed_data):
    cursor = conn.cursor()

    # Read existing services into memory
    cursor.execute('SELECT service, count, countries, filenames FROM services')
    existing_services = {row[0]: row[1:] for row in cursor.fetchall()}

    for service, count in parsed_data['service_counter'].items():
        if service in existing_services:
            # Update existing service data
            new_count = existing_services[service][0] + count
            new_countries = set(existing_services[service][1].split(',')) | {
                c[1] for c in parsed_data['service_country_data'] if c[0] == service}
            new_filenames = existing_services[service][2] + \
                ',' + parsed_data['filename']
            cursor.execute('UPDATE services SET count = $1, countries = $2, filenames = $3 WHERE service = $4',
                           (new_count, ','.join(new_countries), new_filenames, service))
        else:
            # Insert new service
            countries = ','.join(
                {c[1] for c in parsed_data['service_country_data'] if c[0] == service})
            cursor.execute('INSERT INTO services (service, count, countries, filenames) VALUES ($1, $2, $3, $4)',
                           (service, count, countries, parsed_data['filename']))

    conn.commit()


def insert_domain_data(conn, parsed_data):
    cursor = conn.cursor()

    # Read existing domains into memory
    cursor.execute('SELECT domain, count, countries, filenames FROM domains')
    existing_domains = {row[0]: row[1:] for row in cursor.fetchall()}

    for domain, data in parsed_data['domain_counter'].items():
        if domain in existing_domains:
            # Update existing domain data
            new_count = existing_domains[domain][0] + data[0]
            new_countries = set(
                existing_domains[domain][1].split(',')) | set(data[1])
            new_filenames = existing_domains[domain][2] + \
                ',' + parsed_data['filename']
            cursor.execute('UPDATE domains SET count = $1, countries = $2, filenames = $3 WHERE domain = $4',
                           (new_count, ','.join(new_countries), new_filenames, domain))
        else:
            # Insert new domain
            countries = ','.join(data[1])
            cursor.execute('INSERT INTO domains (domain, count, countries, filenames) VALUES ($1, $2, $3, $4)',
                           (domain, data[0], countries, parsed_data['filename']))

    conn.commit()


def insert_summary(conn, parsed_data):
    cursor = conn.cursor()

    # Check if the country already exists in the table
    cursor.execute('SELECT * FROM summary WHERE country = $1',
                   (parsed_data['summary']['country'],))
    result = cursor.fetchone()

    if result:
        # If the country exists, update its values
        unique_usernames = result[1] + \
            parsed_data['summary']['unique_usernames']
        unique_passwords = result[2] + \
            parsed_data['summary']['unique_passwords']
        unique_emails = result[3] + parsed_data['summary']['unique_emails']
        unique_domains = result[4] + parsed_data['summary']['unique_domains']
        unique_services = result[5] + parsed_data['summary']['unique_services']
        cursor.execute('UPDATE summary SET unique_usernames = $1, unique_passwords = $2, unique_emails = $3, unique_domains = $4, unique_services = $5 WHERE country = $6',
                       (unique_usernames, unique_passwords, unique_emails, unique_domains, unique_services, parsed_data['summary']['country']))
    else:
        # If the country does not exist, insert a new row
        cursor.execute('INSERT INTO summary (country, unique_usernames, unique_passwords, unique_emails, unique_domains, unique_services) VALUES ($1, $2, $3, $4, $5, $6)', (parsed_data['summary']['country'], parsed_data[
                       'summary']['unique_usernames'], parsed_data['summary']['unique_passwords'], parsed_data['summary']['unique_emails'], parsed_data['summary']['unique_domains'], parsed_data['summary']['unique_services']))

    conn.commit()


def insert_counts(conn, parsed_data):
    cursor = conn.cursor()

    cursor.execute('INSERT INTO counts (username_count, password_count, email_count, domains_count, services_count, filename) VALUES ($1, $2, $3, $4, $5, $6)',
                   (parsed_data['summary']['unique_usernames'], parsed_data['summary']['unique_passwords'], parsed_data['summary']['unique_emails'], parsed_data['summary']['unique_domains'], parsed_data['summary']['unique_services'], parsed_data['filename']))

    conn.commit()


def process_files(file_range):
    conn = create_database_table()
    optimize_database_indexes(conn)
    try:
        for i in file_range:
            filename = f'database/{str(i).zfill(7)}.json'
            print(filename)
            if os.path.exists(filename):
                print("Current file: ", filename)
                parsed_data = parse_file(filename)
                insert_services_data(conn, parsed_data)
                insert_domain_data(conn, parsed_data)
                insert_summary(conn, parsed_data)
                insert_counts(conn, parsed_data)
            else:
                print(filename, " doesn't exist")
                logging.error(f"File {filename} doesn't exist")
    except Exception as e:
        print(f"Error processing files {filename}: {e}")
        logging.error(f"Error processing files {filename}: {e}")
    finally:
        conn.close()


def main(start_file, end_file, num_processes=8):
    # Create ranges of files for each process
    file_ranges = []
    total_files = end_file - start_file + 1
    chunk_size = total_files // num_processes
    print(chunk_size)

    for i in range(num_processes):
        start = start_file + i * chunk_size
        # Ensure the last chunk gets any remaining files
        end = start_file + \
            (i + 1) * chunk_size if i != num_processes - 1 else end_file + 1
        file_ranges.append(range(start, end))
        print(file_ranges)

    # Create a pool of worker processes
    with Pool(num_processes) as pool:
        pool.map(process_files, file_ranges)


if __name__ == "__main__":
    main(500750, 1809988)  # 0400749
    # main(1, 100)  # Adjust num_processes based on your CPU
