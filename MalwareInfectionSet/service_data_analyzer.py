from sqlmanager import SqlManager
from urllib.parse import urlparse
from collections import defaultdict, Counter
import pycountry
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import fnmatch
from tabulate import tabulate
import tldextract


class ServiceAnalyzerVisualizer:
    """
    A class designed to analyze and visualize data related to various online services, \
        primarily focusing on domain analysis. 
    It integrates functionalities for data fetching, processing, and visualization, \
        utilizing a range of libraries such as pycountry, pandas, seaborn, and matplotlib.

    Attributes:
        sqlmanager (SqlManager): An instance of SqlManager for database interactions.
        data (list): A list to store fetched data.

    Methods:
        __init__: Initializes the ServiceAnalyzerVisualizer instance, setting up the \
            SqlManager and an empty data list.
        get_alpha_2_country_code: Converts a country name to its alpha-2 country code.
        fetch_and_process_data: Fetches service-related data from the database and \
            processes it for analysis.
        get_tld_from_domain: Extracts the top-level domain (TLD) from a given domain name.
            
        top_10_domains_bar_graph: Generates a bar graph of the top 10 domains based \
            on a specified count.
        heatmap_domain_countries: Creates a heatmap visualizing the relationship \
            between domains and countries.
        print_top_domains_and_countries: Prints a tabulated list of the top domains\
            and their associated countries.
        top_tlds: Displays a count of the most common TLDs in the dataset, with an \
            option to filter for a specific TLD.
        service_x_count_relation: Analyzes the relation between services \
            and their associated compromised credentials.
        domain_x_service_relation: Fetches and displays the top services \
            and the domains associated with them.
    """

    def __init__(self):
        self.sqlmanager = SqlManager()
        self.data = []

    def get_alpha_2_country_code(self, country_name):
        # Try to get the country by name
        country = pycountry.countries.get(name=country_name)
        if country:
            return country.alpha_2

        # Try to get the country by official name
        country = pycountry.countries.get(official_name=country_name)
        if country:
            return country.alpha_2

        # If the country is already an alpha_2 code
        if pycountry.countries.get(alpha_2=country_name):
            return country_name

        # Default to 'Unknown'
        return 'Unknown'

    def fetch_and_process_data(self):
        self.data = self.sqlmanager.fetch_all_services()

        # Dictionary to store merged data
        merged_data = defaultdict(
            lambda: {'counter': 0, 'country': set(), 'file_name': set()})

        for row in self.data:
            # Extract domain
            try:
                parsed_url = urlparse(row[0])
                domain = parsed_url.netloc.replace('www.', '')

                counter = row[1]
                countries = set(self.get_alpha_2_country_code(country.strip())
                                for country in row[2].split(','))
                files = set(row[3].split(','))

                # Update the data for the domain
                merged_data[domain]['counter'] += counter
                merged_data[domain]['country'].update(countries)
                merged_data[domain]['file_name'].update(files)
            except ValueError:
                print(f"Skipped: {row[0]}")
                pass

        # Convert set to list for country and file_name, and prepare final data
        final_data = {domain: {'counter': data['counter'],
                               'country': list(data['country']),
                               'file_name': list(data['file_name'])}
                      for domain, data in merged_data.items()}

        return final_data

    def top_10_domains_bar_graph(self, data):
        # Initialize a dictionary to hold domain counts
        domain_counts = {}

        # Aggregate domain counts
        for domain, domain_data in data.items():
            domain_counts[domain] = domain_data['counter']

        # Convert to DataFrame
        df = pd.DataFrame(list(domain_counts.items()),
                          columns=['Domain', 'Count'])

        # Sort and select top 10
        top_10_domains = df.sort_values(by='Count', ascending=False).head(10)

        # Seaborn color palette
        sns.set_palette('viridis')

        # Plot
        plt.figure(figsize=(10, 6))
        sns.barplot(data=top_10_domains, x='Count',
                    y='Domain', palette='viridis')
        plt.title(f'Top 10 Services by Count (Top 10 data point: {top_10_domains["Count"].sum()})')
        plt.xlabel('Count')
        plt.ylabel('Domain')
        plt.show()

    def heatmap_domain_countries(self, data, domain_filter=None, include_unknown=True):
        # Prepare the data
        matrix_data = defaultdict(lambda: defaultdict(int))
        domain_totals = defaultdict(int)

        for domain, domain_data in data.items():
            for country in domain_data['country']:
                # Optionally filter out 'Unknown'
                if not include_unknown and country == 'Unknown':
                    continue
                count = domain_data['counter']
                matrix_data[domain][country] += count
                domain_totals[domain] += count

        # Determine the top 10 domains
        top_10_domains = sorted(
            domain_totals, key=domain_totals.get, reverse=True)[:5]

        # Filter the matrix_data to include only the top 10 domains
        filtered_matrix_data = {
            domain: matrix_data[domain] for domain in top_10_domains}

        # Optionally apply additional domain filtering
        if domain_filter is not None:
            filtered_matrix_data = {domain: matrix for domain, matrix in filtered_matrix_data.items(
            ) if fnmatch.fnmatch(domain, domain_filter)}

        # Convert to DataFrame
        df = pd.DataFrame.from_dict(
            filtered_matrix_data, orient='index').fillna(0)

        # Heatmap
        plt.figure(figsize=(15, 10))
        sns.heatmap(df, annot=True, cmap='viridis')
        total_data_points = sum(domain_totals.values())
        used_data_points = sum(domain_totals[domain]
                               for domain in top_10_domains)
        plt.title(f'Heatmap of Top 10 Domains by Popularity (Total Data Points: {
                  total_data_points}, Used: {used_data_points})')
        plt.xlabel('Country')
        plt.ylabel('Domain')
        plt.show()

    def print_top_domains_and_countries(self, data):
        # Prepare the data
        domain_country_presence = defaultdict(Counter)

        for domain, domain_data in data.items():
            for country in domain_data['country']:
                # Filter out 'Unknown'
                if country == 'Unknown':
                    continue
                domain_country_presence[domain][country] += 1

        # Determine the top 10 domains by total count
        domain_counts = {domain: sum(countries.values())
                         for domain, countries in domain_country_presence.items()}
        top_10_domains = sorted(
            domain_counts, key=domain_counts.get, reverse=True)[:10]

        # Create table for the top 10 domains and their countries
        domains_table = []
        for domain in top_10_domains:
            countries = [pycountry.countries.get(
                alpha_2=code).name for code in domain_country_presence[domain] if pycountry.countries.get(alpha_2=code)]
            domains_table.append(
                [domain, len(countries), ', '.join(countries)])

        print(tabulate(domains_table, headers=[
              'Domain', 'Total Countries', 'Countries'], tablefmt='grid'))

        # Find the most common countries across the top 10 domains
        common_countries = Counter()
        for domain in top_10_domains:
            common_countries.update(domain_country_presence[domain])

        # Create table for the most common countries
        countries_table = []
        for country, count in common_countries.most_common(100):
            country_name = pycountry.countries.get(
                alpha_2=country).name if pycountry.countries.get(alpha_2=country) else country
            countries_table.append([country_name, count])

        print("\nMost Common Countries Across Top 100 Domains:")
        print(tabulate(countries_table, headers=[
              'Country', 'Appearances'], tablefmt='grid'))

    def get_tld_from_domain(self, domain):
        extracted = tldextract.extract(domain)
        # Join the TLD and suffix (if any, like co.uk)
        return f".{extracted.suffix}" if extracted.suffix else ''

    def top_tlds(self, data, specific_tld=None):
        tld_counter = Counter()

        for domain, domain_data in data.items():
            tld = self.get_tld_from_domain(domain)
            # If a specific TLD is provided, match against that TLD
            if len(tld) != 0:
                if specific_tld:
                    if tld == specific_tld or tld.endswith(specific_tld):
                        tld_counter[tld] += 1
                else:
                    tld_counter[tld] += 1

        # Prepare data for the table
        if specific_tld is None:
            tld_table = [[tld, count]
                         for tld, count in tld_counter.most_common(20)]
        else:
            tld_table = [[tld, count]
                         for tld, count in tld_counter.most_common()]

        # Print the table
        print(tabulate(tld_table, headers=['TLD', 'Count'], tablefmt='grid'))

    def service_x_count_relation(self, data, domain_pattern):
        """
        Try to figure out Services and their associated compromised credentails.
        There might be mismatches of data here as it's not confirmed which credentials are from which
        services
        """
        # Prepare the data
        matching_filenames = []
        for domain, domain_data in data.items():
            # Check for full match or suffix match
            if fnmatch.fnmatch(domain, domain_pattern):
                matching_filenames.extend(domain_data['file_name'])

        # Convert list to a format suitable for SQL IN clause
        filenames_tuple = tuple(matching_filenames)

        # If there are no matching filenames, there's nothing to fetch, so return
        if not filenames_tuple:
            print("No data available for the specified domain pattern.")
            return

        # Fetch the counts for these filenames from the database
        counts = self.sqlmanager.filter_service_x_count(filenames_tuple)

        # Aggregate counts
        aggregated_counts = {'email': 0, 'domains': 0,
                             'username': 0, 'password': 0}
        for row in counts:
            aggregated_counts['email'] += row[0]
            aggregated_counts['domains'] += row[1]
            aggregated_counts['username'] += row[2]
            aggregated_counts['password'] += row[3]

        final_data = [(domain_pattern, aggregated_counts['email'],
                       aggregated_counts['username'], aggregated_counts['password'])]

        # Print out the fetched data
        print(tabulate(final_data, headers=[
              'Domain/Domain Pattern', 'Email Count', 'Username Count', 'Password Count'], tablefmt='grid'))

    def domain_x_service_relation(self, data):
        """
        Fetch top 10 services, and all the domains that are compromised under those services
        """
        # Extract service and counter into a list of tuples
        service_counters = [(service, details['counter'], details['file_name'])
                            for service, details in data.items()]

        # Sort the list of tuples by counter in descending order
        sorted_service_counters = sorted(
            service_counters, key=lambda x: x[1], reverse=True)

        # Get the top 10 domains by counter
        top_10_services = sorted_service_counters[:10]

        # You can now use top_10_domains for further processing or print it out
        print("Top 10 Services and the domains they were associated with")
        for service, counter, filenames in top_10_services:
            compromised_domain = self.sqlmanager.filter_service_x_domains(
                filenames=filenames)
            print(service, len(compromised_domain))


if __name__ == "__main__":
    malwareanalyzer = ServiceAnalyzerVisualizer()
    data = malwareanalyzer.fetch_and_process_data()
    # malwareanalyzer.top_10_domains_bar_graph(data)
    # malwareanalyzer.print_top_domains_and_countries(data)
    # malwareanalyzer.top_tlds(data, specific_tld=None)
    # malwareanalyzer.domain_x_service_relation(data)
    malwareanalyzer.service_x_count_relation(data, "*.fi")
