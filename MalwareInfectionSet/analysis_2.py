import json
import os
import sqlite3
import tldextract
from multiprocessing import Pool
import logging


logging.basicConfig(filename='analyzer_logger.log',
                    level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')


class MalwareAnalyzer:
    """
    The MalwareAnalyzer class is designed for processing and analyzing malware-related data. 
    It handles the parsing of JSON files containing various malware attributes and stores the 
    processed data in a SQLite database. The class provides functionalities to create and optimize 
    database tables, extract and summarize data from files, and insert the processed data into 
    the database. It is equipped to handle large datasets efficiently using multiprocessing.

    Attributes:
        db_name (str): Name of the SQLite database file.
        conn (sqlite3.Connection): SQLite database connection object.

    Methods:
        create_database_table(): Creates necessary tables in the database.
        optimize_database_indexes(): Optimizes database queries by creating indexes.
        parse_file(filename): Parses a given JSON file and extracts relevant malware data.
        insert_services_data(parsed_data): Inserts or updates service-related data into the database.
        insert_domain_data(parsed_data): Inserts or updates domain-related data into the database.
        insert_summary(parsed_data): Inserts or updates summary data into the database.
        insert_counts(parsed_data): Inserts counts of various data types into the database.
        process_file(filename): Processes a single file, invoking parsing and data insertion.
        close_connection(): Closes the database connection.

    Usage:
        This class is primarily used in scenarios where there is a need to process and analyze
        a large number of files containing malware data, such as service usage, domain names,
        and user credentials. It facilitates the storage and subsequent analysis of this data
        in a structured format.

    Note:
        The class is designed to work with specific JSON file structures and database schema. 
        Modifications may be necessary to adapt to different data formats or database designs.
    """

    def __init__(self, db_name):
        self.db_name = db_name
        self.conn = None

    def create_database_table(self):
        self.conn = sqlite3.connect(self.db_name)
        cursor = self.conn.cursor()

        # Create tables
        cursor.execute('''CREATE TABLE IF NOT EXISTS services (
                        service TEXT PRIMARY KEY,
                        count INTEGER,
                        countries TEXT,
                        filenames TEXT)''')

        cursor.execute('''CREATE TABLE IF NOT EXISTS domains (
                        domain TEXT PRIMARY KEY,
                        count INTEGER,
                        countries TEXT,
                        filenames TEXT)''')

        cursor.execute('''CREATE TABLE IF NOT EXISTS summary (
                        country TEXT,
                        unique_usernames INTEGER,
                        unique_passwords INTEGER,
                        unique_emails INTEGER,
                        unique_domains INTEGER,
                        unique_services INTEGER
                        )''')

        cursor.execute('''CREATE TABLE IF NOT EXISTS counts (
                        username_count INTEGER,
                        password_count INTEGER,
                        email_count INTEGER,
                        domains_count INTEGER,
                        services_count INTEGER,
                        filename TEXT)''')
        self.conn.commit()

    def optimize_database_indexes(self):
        cursor = self.conn.cursor()

        # Index for the 'services' table
        cursor.execute(
            'CREATE INDEX IF NOT EXISTS idx_services_service ON services (service)')
        cursor.execute(
            'CREATE INDEX IF NOT EXISTS idx_services_countries ON services (countries)')
        cursor.execute(
            'CREATE INDEX IF NOT EXISTS idx_services_filenames ON services (filenames)')

        # Index for the 'domains' table
        cursor.execute(
            'CREATE INDEX IF NOT EXISTS idx_domains_domain ON domains (domain)')
        cursor.execute(
            'CREATE INDEX IF NOT EXISTS idx_domains_countries ON domains (countries)')
        cursor.execute(
            'CREATE INDEX IF NOT EXISTS idx_domains_filenames ON domains (filenames)')

        # Index for the 'summary' table
        cursor.execute(
            'CREATE INDEX IF NOT EXISTS idx_summary_country ON summary (country)')

        # Index for the 'counts' table
        cursor.execute(
            'CREATE INDEX IF NOT EXISTS idx_counts_filename ON counts (filename)')
        self.conn.commit()

    def parse_file(self, filename):
        with open(filename, 'r') as file:
            data = json.load(file)

        # Initialize containers for the data to be extracted
        service_counter = {}
        domain_counter = {}
        service_country_data = []
        unique_usernames = set()
        unique_passwords = set()
        unique_emails = set()
        unique_domains = set()

        # Parse the data
        country = data.get('country', 'Unknown')

        # Count services and collect country data
        for service in data.get('services', []):
            service_counter[service] = service_counter.get(service, 0) + 1
            service_country_data.append((service, country))

        # Count domains and collect country data
        for domain in data.get('domain_names', []):
            # Extract the parent domain using tldextract
            domain_parts = tldextract.extract(domain)
            parent_domain = f"{domain_parts.domain}.{domain_parts.suffix}"

            # Update the domain counter
            if parent_domain in domain_counter:
                domain_counter[parent_domain][0] += 1
                if country not in domain_counter[parent_domain][1]:
                    domain_counter[parent_domain][1].append(country)
            else:
                domain_counter[parent_domain] = [1, [country]]

        # Collect unique usernames, passwords, emails, and domains
        unique_usernames.update(data.get('usernames', []))
        unique_passwords.update(data.get('passwords', []))
        unique_emails.update(data.get('email_addresses', []))
        unique_domains.update(domain_counter.keys())

        # Prepare the parsed data for return
        parsed_data = {
            'service_counter': service_counter,
            'domain_counter': domain_counter,
            'service_country_data': service_country_data,
            'summary': {
                'country': country,
                'unique_usernames': len(unique_usernames),
                'unique_passwords': len(unique_passwords),
                'unique_emails': len(unique_emails),
                'unique_domains': len(unique_domains),
                'unique_services': len(service_counter)
            },
            'filename': os.path.basename(filename)
        }

        return parsed_data

    def insert_services_data(self, parsed_data):
        cursor = self.conn.cursor()

        # Read existing services into memory
        cursor.execute(
            'SELECT service, count, countries, filenames FROM services')
        existing_services = {row[0]: row[1:] for row in cursor.fetchall()}

        for service, count in parsed_data['service_counter'].items():
            if service in existing_services:
                # Update existing service data
                new_count = existing_services[service][0] + count
                new_countries = set(existing_services[service][1].split(',')) | {
                    c[1] for c in parsed_data['service_country_data'] if c[0] == service}
                new_filenames = existing_services[service][2] + \
                    ',' + parsed_data['filename']
                cursor.execute('UPDATE services SET count = $1, countries = $2, filenames = $3 WHERE service = $4',
                               (new_count, ','.join(new_countries), new_filenames, service))
            else:
                # Insert new service
                countries = ','.join(
                    {c[1] for c in parsed_data['service_country_data'] if c[0] == service})
                cursor.execute('INSERT INTO services (service, count, countries, filenames) VALUES ($1, $2, $3, $4)',
                               (service, count, countries, parsed_data['filename']))

        self.conn.commit()

    def insert_domain_data(self, parsed_data):
        cursor = self.conn.cursor()

        # Read existing domains into memory
        # this is an expensive operation. I should have optimized it.
        cursor.execute(
            'SELECT domain, count, countries, filenames FROM domains')
        existing_domains = {row[0]: row[1:] for row in cursor.fetchall()}

        for domain, data in parsed_data['domain_counter'].items():
            if domain in existing_domains:
                # Update existing domain data
                new_count = existing_domains[domain][0] + data[0]
                new_countries = set(
                    existing_domains[domain][1].split(',')) | set(data[1])
                new_filenames = existing_domains[domain][2] + \
                    ',' + parsed_data['filename']
                cursor.execute('UPDATE domains SET count = $1, countries = $2, filenames = $3 WHERE domain = $4',
                               (new_count, ','.join(new_countries), new_filenames, domain))
            else:
                # Insert new domain
                countries = ','.join(data[1])
                cursor.execute('INSERT INTO domains (domain, count, countries, filenames) VALUES ($1, $2, $3, $4)',
                               (domain, data[0], countries, parsed_data['filename']))

        self.conn.commit()

    def insert_summary(self, parsed_data):
        cursor = self.conn.cursor()

        # Check if the country already exists in the table
        cursor.execute('SELECT * FROM summary WHERE country = $1',
                       (parsed_data['summary']['country'],))
        result = cursor.fetchone()

        if result:
            # If the country exists, update its values
            unique_usernames = result[1] + \
                parsed_data['summary']['unique_usernames']
            unique_passwords = result[2] + \
                parsed_data['summary']['unique_passwords']
            unique_emails = result[3] + parsed_data['summary']['unique_emails']
            unique_domains = result[4] + \
                parsed_data['summary']['unique_domains']
            unique_services = result[5] + \
                parsed_data['summary']['unique_services']
            cursor.execute('UPDATE summary SET unique_usernames = $1, unique_passwords = $2, unique_emails = $3, unique_domains = $4, unique_services = $5 WHERE country = $6',
                           (unique_usernames, unique_passwords, unique_emails, unique_domains, unique_services, parsed_data['summary']['country']))
        else:
            # If the country does not exist, insert a new row
            cursor.execute('INSERT INTO summary (country, unique_usernames, unique_passwords, unique_emails, unique_domains, unique_services) VALUES ($1, $2, $3, $4, $5, $6)', (parsed_data['summary']['country'], parsed_data[
                'summary']['unique_usernames'], parsed_data['summary']['unique_passwords'], parsed_data['summary']['unique_emails'], parsed_data['summary']['unique_domains'], parsed_data['summary']['unique_services']))

        self.conn.commit()

    def insert_counts(self, parsed_data):
        cursor = self.conn.cursor()

        cursor.execute('INSERT INTO counts (username_count, password_count, email_count, domains_count, services_count, filename) VALUES ($1, $2, $3, $4, $5, $6)',
                       (parsed_data['summary']['unique_usernames'], parsed_data['summary']['unique_passwords'], parsed_data['summary']['unique_emails'], parsed_data['summary']['unique_domains'], parsed_data['summary']['unique_services'], parsed_data['filename']))

        self.conn.commit()

    def process_file(self, filename):
        if not self.conn:
            self.conn = sqlite3.connect(self.db_name)
        logging.info(f"Processing file: {filename}")
        print(f"Processing file: {filename}")
        try:
            self.optimize_database_indexes()
            parsed_data = self.parse_file(filename)
            self.insert_services_data(parsed_data)
            self.insert_domain_data(parsed_data)
            self.insert_summary(parsed_data)
            self.insert_counts(parsed_data)
            print(f"Successfully processed file: {filename}")
            logging.info(f"Successfully processed file: {filename}")
        except Exception as e:
            print(f"Error processing file {filename}: {e}")
            logging.error(f"Error processing file {filename}: {e}")

    def close_connection(self):
        if self.conn:
            self.conn.close()
            self.conn = None


def process_files_wrapper(file_range, db_name):
    analyzer = MalwareAnalyzer(db_name)
    analyzer.create_database_table()
    analyzer.optimize_database_indexes()
    try:
        for filename in file_range:
            if os.path.exists(filename):
                analyzer.process_file(filename)
            else:
                print(f"File {filename} doesn't exist")
                logging.warning(f"File {filename} doesn't exist")
    except Exception as e:
        logging.error(f"Error processing files {filename}: {e}")
    finally:
        analyzer.close_connection()


def main(start_file, end_file, num_processes=8, db_name='001malware.db'):
    file_ranges = []
    total_files = end_file - start_file + 1
    chunk_size = total_files // num_processes

    # Update this line to point to the new directory
    base_directory = 'F:\\Digital Security\\malwarevictims\\database\\'

    for i in range(num_processes):
        start = start_file + i * chunk_size
        end = start_file + \
            (i + 1) * chunk_size if i != num_processes - 1 else end_file + 1
        file_ranges.append(
            [base_directory + f'{str(j).zfill(7)}.json' for j in range(start, end)])

    with Pool(num_processes) as pool:
        pool.starmap(process_files_wrapper, [
                     (file_range, db_name) for file_range in file_ranges])


if __name__ == "__main__":
    # main(500750, 1809988)
    main(500750, 1000000)
